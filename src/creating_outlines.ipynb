{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "from common.path_setup import data_dir\n",
    "\n",
    "# Path to the PDF file\n",
    "pdf_path = os.path.join(data_dir,'electoral_rolls.pdf')\n",
    "\n",
    "# Convert the first page of the PDF to an image\n",
    "images = convert_from_path(pdf_path, first_page=3, last_page=3)\n",
    "\n",
    "\n",
    "# Save the image of the first page\n",
    "image_path = os.path.join(data_dir, 'pdf_images', 'page_3.png')\n",
    "images[0].save(image_path, 'PNG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "[93517.5, 93520.5, 93539.0, 93539.0, 93544.0, 93546.5, 93634.5, 93643.0, 93654.5, 93657.5, 93667.0, 93667.0, 93911.0, 93935.0, 94011.5, 94014.0, 94015.0, 94034.5, 94034.5, 94035.5, 94038.5, 94039.0, 94115.0, 94136.5, 94136.5, 94435.0, 94515.0, 94534.5, 94535.5, 94536.0]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import ImageEnhance, ImageFilter\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "\n",
    "# Convert to gray scale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Use adaptive thresholding to highlight the regions of interest\n",
    "# This method is effective in varying lighting conditions and contrasting background\n",
    "thresh = cv2.adaptiveThreshold(\n",
    "    gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)\n",
    "\n",
    "# Find contours on the thresholded image\n",
    "contours, _ = cv2.findContours(\n",
    "    thresh, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Filter contours based on area to remove noise\n",
    "filtered_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 30000 and cv2.contourArea(cnt) < 250000]\n",
    "areas = [cv2.contourArea(cnt) for cnt in filtered_contours]\n",
    "# We'll store the bounding boxes of the regions of interest here\n",
    "bounding_boxes = []\n",
    "\n",
    "# For each contour, find the bounding rectangle and store it\n",
    "for contour in filtered_contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    bounding_boxes.append((x, y, x+w, y+h))\n",
    "\n",
    "    # Draw a rectangle around the contour for visualization\n",
    "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "# Save the image with outlined regions of interest\n",
    "outlined_image_path = os.path.join(data_dir, 'pdf_images', 'outlined_images','page_2_final.png')\n",
    "cv2.imwrite(outlined_image_path, image)\n",
    "\n",
    "print(len(filtered_contours))\n",
    "print(sorted(areas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Region_0': 'Photo is\\nAvailable',\n",
       " 'Region_1': 'Photo is\\nAvailable',\n",
       " 'Region_2': 'Photo is\\nAvailable'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "# Initialize a dictionary to hold the OCR results\n",
    "ocr_results = {}\n",
    "\n",
    "# Function to extract text from an ROI in the image\n",
    "def ocr_on_roi(image, contour, index):\n",
    "    # Get the bounding box for the contour\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "\n",
    "    # Extract the region of interest\n",
    "    roi = image[y:y+h, x:x+w]\n",
    "\n",
    "    # Use Tesseract to do OCR on the image\n",
    "    text = pytesseract.image_to_string(roi, config='--psm 6', output_type=Output.STRING)\n",
    "\n",
    "    return text.strip()  # Remove any leading/trailing white space\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Process a reasonable number of regions (not all, to save time)\n",
    "# and perform OCR on each\n",
    "max_regions = 5  # Maximum number of regions to process for demonstration\n",
    "for i, contour in enumerate(filtered_contours[2:max_regions]):\n",
    "    text = ocr_on_roi(gray_image, contour, i)  # Use grayscale image for OCR\n",
    "    if text:  # If text was detected, save it with an identifier\n",
    "        ocr_results[f\"Region_{i}\"] = text\n",
    "\n",
    "ocr_results  # Display the results from the OCR process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
